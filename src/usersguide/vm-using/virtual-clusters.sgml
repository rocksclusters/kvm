<section id="using-virtual-clusters" xreflabel="Installing Virtual Clusters">
	<title>Installing Virtual Clusters</title>

<section id="provisioning-virtual-cluster"
	xreflabel="Provisioning a Virtual Cluster">
	<title>Provisioning a Virtual Cluster</title>

<para>
After you install a VM Server and at least one VM Container, you are ready
to provision a virtual cluster.
</para>

<para>
We'll use the following illustration as a guide to help keep track of the
names of the physical machines and the virtual machines.
</para>

<para>
<mediaobject>
	<imageobject>
		<imagedata fileref="images/virtual-cluster.png" scale=50>
	</imageobject>
</mediaobject>
</para>

<para>
In the above picture, "espresso.rocksclusters.org" is a physical machine.
Also, "vm-container-0-0" and "vm-container-0-1" are physical machines that
were kickstarted by "espresso".
The machine "frontend-0-0-0" is a virtual machine that is hosted by
"espresso".
The machines "hosted-vm-0-0-0" and "hosted-vm-0-1-0" are VMs that are
associated with "frontend-0-0-0" (they are all in the same VLAN).
</para>

<para>
Depending on your perspective, the virtual machines have different names.
Dom0 is a physical machine that hosts (multiple) virtual systems.
DomU are guests and generally refer to names by usual convention.
The equivalence is:

<table>
<title>Virtual Machine Names</title>
<tgroup cols="3">
	<thead>
	<row>
		<entry>Host</entry>
		<entry>Dom0 Name (physical)</entry>
		<entry>DomU Name (virtual)</entry>
	</row>
	</thead>

	<tbody>
	<row>
		<entry>37:77:6e:c0:00:00</entry>
		<entry>frontend-0-0-0</entry>
		<entry>vi-1.rocksclusters.org</entry>
	</row>

	<row>
		<entry>37:77:6e:c0:00:01</entry>
		<entry>hosted-vm-0-0-0</entry>
		<entry>compute-0-0</entry>
	</row>

	<row>
		<entry>37:77:6e:c0:00:02</entry>
		<entry>hosted-vm-0-1-0</entry>
		<entry>compute-0-1</entry>
	</row>
	</tbody>

</tgroup>
</table>

</para>

<note>
<para>
An important point is that the only common thing between the physical side
and the virtual side is the MAC address (in yellow).
We will use the MAC address of a virtual machine to control it (e.g., to
initially power it on).
</para>
</note>

<para>
The names in the virtual cluster look like the names in a traditional
cluster -- the frontend is named "vi-1.rocksclusters.org" and its compute
nodes are named "compute-0-0" and "compute-0-1".
If you login to "vi-1.rocksclusters.org", you would be hard pressed to tell
the difference between this virtual cluster and a traditional physical cluster.
</para>

<warning>
<para>
You must select your own IP address for your virtual frontend.
The IP address "137.110.119.118" is managed by UCSD and should not be used by
you.
</para>

<para>
They are only used here to show you a concrete example.
</para>
</warning>

<para>
First, we'll add a virtual cluster to the VM Server's database.
In this example, we'll add a frontend with the IP of "137.110.119.118"
and we'll associate 2 compute nodes with it:
</para>

<screen>
# rocks add cluster ip="137.110.119.118" num-computes=2
</screen>

<para>
The above command will take some time and then output something similar to:
</para>

<screen>
created frontend VM named: frontend-0-0-0 
	created compute VM named: hosted-vm-0-0-0
	created compute VM named: hosted-vm-0-1-0
</screen>

<para>
The command adds entries to the database for the above nodes and establishes
a VLAN that will be used for the private network (eth0 inside the VM).
</para>

<para>
Info about all the defined clusters on the VM Server (including the
physical cluster) can be obtained with the command:
<computeroutput>rocks list cluster</computeroutput>:
</para>

<screen>
# rocks list cluster
FRONTEND                    CLIENT NODES     TYPE    
espresso.rocksclusters.org: ---------------- physical
:                           vm-container-0-0 physical
:                           vm-container-0-1 physical
frontend-0-0-0-public:      ---------------- VM      
:                           hosted-vm-0-0-0  VM      
:                           hosted-vm-0-1-0  VM
</screen>

</section>


<section id="airboss"
	xreflabel="The Airboss">
	<title>The Airboss</title>

<para>
In Rocks, we've developed a service known as the "Airboss" that resides
on the physical frontend (in Dom0) and it allows non-root users to control 
their VMs.
The motivation for this service is that libvirt (a virtualization API written
by RedHat that can control several different virtualization implementations)
assumes "root" access to control and monitor VMs.
</para>

<para>
The Airboss in Rocks is a small service that uses digitally signed
messages to give non-root users access to their virtual cluster (and
only their virtual cluster). 
The Airboss relies upon public/private key pairs to validate messages.
The administrator of the physical hosting cluster must issue a single command
to associate a public key with a particular virtual cluster.
At that point, the full process of booting and installing a virtual cluster
can be controlled by the (authorized) non-root user.
</para>

<para>
<mediaobject>
	<imageobject>
		<imagedata fileref="images/airboss.png" scale=50>
	</imageobject>
</mediaobject>
</para>

<para>
In the above picture, a user that is logged in to vi-1.rocksclusters.org
wants to power on compute-0-0 (one of the VMs associated with the virtual
cluster).
The user executes the "power on" command.
The command creates a "power on" message, signs it with a private key, then
sends it to the Airboss that is running on espresso.rocksclusters.org.
The Airboss verifies the message signature.
If the signature is valid, then the Airboss instructs libvirt on
vm-container-0-0 to start ("power on") compute-0-0.
</para>

</section>


<section id="creating-keys"
	xreflabel="Creating an RSA Key Pair">
	<title>Creating an RSA Key Pair</title>

<para>
Before we can install a VM, we must create an RSA key pair.
These keys will be used to authenticate Airboss commands.
To create a key pair, execute:
</para>

<screen>
# rocks create keys key=private.key
</screen>

<para>
The above command will ask for a pass phrase for the private key.
If you would like a "passphraseless" private key, execute:
</para>

<screen>
# rocks create keys key=private.key passphrase=no
</screen>

<para>
The above command will place your private key into the file private.key
and it will output the public key for your private key:
</para>

<screen>
# rocks create keys key=private.key
Generating RSA private key, 1024 bit long modulus
............++++++
.......++++++
e is 65537 (0x10001)
Enter pass phrase for private.key:
Verifying - Enter pass phrase for private.key:
Enter pass phrase for private.key:
writing RSA key
-----BEGIN PUBLIC KEY-----
MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDMoCPmR/Kev64znRBxvtsniXIF
dyQMxR/bBFKNDmvmzPuPUim5jmD3TLilnH75/KidtJCwlb+Lhr5Cs6/9sRzX6rX2
ExVUZsgo4A+O+XMk8KeowO/c2rPc+YdXaBir3Aesm/MCfCZaidZae8QLmVKW7Va5
qErl9gyhhR7uDX+hgwIDAQAB
-----END PUBLIC KEY-----
</screen>

<para>
Now save the public key to file, that is, copy the above public key:
</para>

<screen>
-----BEGIN PUBLIC KEY-----
MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDMoCPmR/Kev64znRBxvtsniXIF
dyQMxR/bBFKNDmvmzPuPUim5jmD3TLilnH75/KidtJCwlb+Lhr5Cs6/9sRzX6rX2
ExVUZsgo4A+O+XMk8KeowO/c2rPc+YdXaBir3Aesm/MCfCZaidZae8QLmVKW7Va5
qErl9gyhhR7uDX+hgwIDAQAB
-----END PUBLIC KEY-----
</screen>

<para>
And save your public key into a file (e.g., $HOME/public.key).
</para>

<para>
We now want to associate your public key with the virtual cluster you
provisioned.
This will allow you to use your private key to send authenticated commands
to control your cluster.
To associate your public key with your virtual cluster, execute:
</para>

<screen>
# rocks add host key frontend-0-0-0 key=public.key
</screen>

<para>
We can see the relationship by executing:
</para>

<screen>
# rocks list host key
HOST            ID PUBLIC KEY                                                      
frontend-0-0-0: 7  -----BEGIN PUBLIC KEY-----                                      
:                  MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDMoCPmR/Kev64znRBxvtsniXIF
:                  dyQMxR/bBFKNDmvmzPuPUim5jmD3TLilnH75/KidtJCwlb+Lhr5Cs6/9sRzX6rX2
:                  ExVUZsgo4A+O+XMk8KeowO/c2rPc+YdXaBir3Aesm/MCfCZaidZae8QLmVKW7Va5
:                  qErl9gyhhR7uDX+hgwIDAQAB                                        
:                  -----END PUBLIC KEY-----                                        
:                  ----------------------------------------------------------------
</screen>

<para>
We see that the public key is associated with "frontend-0-0-0" (the name of
the VM in Dom0).
</para>

</section>


<section id="using-virtual-clusters-frontend"
	xreflabel="Installing a VM Frontend">
	<title>Installing a VM Frontend</title>

<para>
Now, we'll want to install the virtual frontend.
First, login to the physical frontend (e.g., espresso).
To start the VM frontend install, we'll need to power on and install the VM
frontend:
</para>

<screen>
# rocks set host power frontend-0-0-0 action=install key=private.key
</screen>

<note>
<para>
The action of "install" ensures that the VM will be put into install mode,
then it will be powered on.
</para>
</note>

<para>
Then, to connect to the VM's console, execute:
</para>

<screen>
# rocks open host console frontend-0-0-0 key=private.key
</screen>

<para>
Soon you will see the familiar frontend installation screen:
</para>

<para>
<mediaobject>
	<imageobject>
		<imagedata fileref="images/vm-frontend.png" scale=50>
	</imageobject>
</mediaobject>
</para>

<para>
In the "Hostname of Roll Server" field, insert the FQDN of your VM Server
(the name of the physical machine that is hosting the VM frontend).
Then click "Download".
</para>

<para>
From here, you want to follow the
<ulink url="/roll-documentation/base/&document-version;/install-frontend.html">standard procedure for bringing up a frontend</ulink> starting at Step 8.
</para>

<para>
After the VM frontend installs, it will reboot.
After it reboots, login and then we'll begin installing VM compute nodes.
</para>

</section>


<section id="using-virtual-clusters-compute"
	xreflabel="Installing VM Compute Nodes">
	<title>Installing VM Compute Nodes</title>

<para>
Login to the VM frontend (the virtual machine named "vi-1.rocksclusters.org"
in the example picture at the top of this page), and execute:
</para>

<screen>
# insert-ethers
</screen>

<para>
Select "Compute" as the appliance type.
</para>

<para>
In another terminal session on vi-1.rocksclusters.org, we'll need to set up
the environment to send commands to the Airboss on the physical frontend.
We'll do this by putting the RSA private key that we created in section
<xref linkend="creating-keys"> (e.g., private.key) on vi-1.rocksclusters.org.
</para>

<para>
Prior to sending commands to the Airboss, we need to establish a ssh tunnel
between the virtual frontend (e.g., vi-1) and the physical frontend (e.g.,
espresso, where the Airboss runs).
This tunnel is used to securely pass Airboss messages.
On the virtual frontend (e.g., vi-1), execute:
</para>

<screen>
# ssh -L 8677:localhost:8677 espresso.rocksclusters.org
</screen>

<para>
Now we can securely send messages to the Airboss.
</para>

<para>
Now, we're ready to install compute nodes.
But, there's a problem - when we first login to vi-1.rocksclusters.org, the
only machine we know about is ourself (i.e., vi-1.rocksclusters.org).
There are no other nodes in the virtual frontend's database.
But the physical machine knows about the MAC addresses of the virtual
compute nodes (e.g., hosted-vm-0-0-0 and hosted-vm-0-1-0) that are associated
with this virtual cluster.
The good news is, we can ask the Airboss on the physical frontend for a
list of MAC addresses that are assigned to our virtual cluster:
</para>

<screen>
# rocks list host macs vi-1.rocksclusters.org key=private.key 
</screen>

<para>
Which outputs:
</para>

<screen>
MACS IN CLUSTER  
36:77:6e:c0:00:02
36:77:6e:c0:00:00
36:77:6e:c0:00:03
</screen>

<para>
The MAC address 36:77:6e:c0:00:00 is ourself (the VM frontend) and the other
two MACs (36:77:6e:c0:00:02 and 36:77:6e:c0:00:03) are the VM compute nodes
that are associated with our VM frontend.
</para>

<para>
We can use the MAC address of the VM compute nodes to power up and install our
compute nodes:
</para>

<screen>
# rocks set host power 36:77:6e:c0:00:02 key=private.key action=install
</screen>

<note>
<para>
The action of "install" ensures that the VM will be put into install mode,
then it will be powered on.
</para>
</note>

<para>
Soon, you should see insert-ethers discover the VM compute node:
</para>

<para>
<mediaobject>
	<imageobject>
		<imagedata fileref="images/discovered.png" scale=50>
	</imageobject>
</mediaobject>
</para>

<para>
After the virtual compute node is discovered by insert-ethers, we can open
a console to the node by executing:
</para>

<screen>
# rocks open host console compute-0-0 key=private.key
</screen>

<para>
Lastly, to power off a virtual compute node (e.g., compute-0-0), execute:
</para>

<screen>
# rocks set host power compute-0-0 key=private.key action=off
</screen>

</section>


<section id="using-virt-manager"
	xreflabel="Using RedHat's Virt-Manager (Root Users Only)">
	<title>Using RedHat's Virt-Manager (Root Users Only)</title>

<para>
Virt-manager is a program produced by RedHat that is a desktop user interface
for managing virtual machines.
This section describes how to use some of virt-manager's features to control
and monitor VMs on a Rocks cluster.
</para>

<para>
To interact with the VM frontend's console, on the physical frontend, you
need to start "virt-manager":
</para>

<screen>
# virt-manager
</screen>

<para>
This will display a screen similar to:
</para>

<para>
<mediaobject>
	<imageobject>
		<imagedata fileref="images/virt-manager-1.png" scale=50>
	</imageobject>
</mediaobject>
</para>

<para>
Double click on the "localhost" entry and then you'll see:
</para>

<para>
<mediaobject>
	<imageobject>
		<imagedata fileref="images/virt-manager-2.png" scale=50>
	</imageobject>
</mediaobject>
</para>

<para>
To bring the up the console for the VM frontend, double click on
"frontend-0-0-0".
</para>

<para>
Now we'll describe how to connect to the console for the virtual compute
node "compute-0-0".
In the example configuration described at the top of this page,
the VM "compute-0-0" is hosted on the physical machine named
"vm-container-0-0" so we'll need to tell "virt-manager" to open a connection
to "vm-container-0-0".
</para>

<para>
Inside "virt-manager", click on "File" then "Open connection...".
This brings up a window that looks like:
</para>

<para>
<mediaobject>
	<imageobject>
		<imagedata fileref="images/virt-manager-10.png" scale=50>
	</imageobject>
</mediaobject>
</para>

<para>
Now change the "Connection:" field to "Remote tunnel over SSH" and enter
"vm-container-0-0" for the "Hostname:" field:
</para>

<para>
<mediaobject>
	<imageobject>
		<imagedata fileref="images/virt-manager-11.png" scale=50>
	</imageobject>
</mediaobject>
</para>

<para>
Then click "Connect".
</para>

<para>
In the "virt-manager" window, you should see something similar to:
</para>

<para>
<mediaobject>
	<imageobject>
		<imagedata fileref="images/virt-manager-12.png" scale=50>
	</imageobject>
</mediaobject>
</para>

<para>
Double click on "vm-container-0-0" and then you'll see:
</para>

<para>
<mediaobject>
	<imageobject>
		<imagedata fileref="images/virt-manager-13.png" scale=50>
	</imageobject>
</mediaobject>
</para>

<para>
Now to connect to the compute node's console, double click on "hosted-vm-0-0-0".
Recall that from the perspective of the physical frontend (the VM Server),
"hosted-vm-0-0-0" is the name for the VM "compute-0-0" (again, see the
figure at the top of this page).
</para>

<para>
You should now see the console for compute-0-0:
</para>

<para>
<mediaobject>
	<imageobject>
		<imagedata fileref="images/virt-manager-14.png" scale=50>
	</imageobject>
</mediaobject>
</para>

</section>

</section>

